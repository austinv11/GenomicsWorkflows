"""
Snakemake workflow for scRNA-seq analysis incorporating shortread preprocessing.

This workflow combines short read processing with downstream scRNA-seq analysis.
"""

# Set default config values if not provided
config.setdefault("registry", "biohpc_aav4003")
config.setdefault("fastq_dir","raw_data")
config.setdefault("results_dir","results")
config.setdefault("low_complexity_filter",True)

# Single-cell specific config values
config.setdefault("mode", 'three-prime')  # Either three-prime or flex
config.setdefault("organism", 'human')  # Either human or mouse
config.setdefault("cellranger_output", "cellranger_out")  # Cellranger output directory
config.setdefault("outlier_threshold", "5")  # Outlier threshold for preprocessing
config.setdefault("n_hvgs", "5000")  # Number of highly variable genes to select

# Import shortread workflow
include: "../rules/shortread.smk"

# Import required functions
from pathlib import Path
import glob
import re


def get_samples():
    return sorted({
        re.sub(r"_L\d{3}(_R[12].*)?$", "", Path(f).stem)   # strip lane & read
                   for f in glob.glob(f"{config['fastq_dir']}/*_R1_*.f*q*")})

# Update the all rule to ensure it requests all samples
rule all:
    input:
        expand(
            f"{os.path.abspath(config['results_dir'])}/multiqc/{{sample}}/multiqc_report.html",
            sample=get_samples()
        ),
        expand(
            f"{os.path.abspath(config['results_dir'])}/{config['cellranger_output']}/{{sample}}",
            sample=get_samples()
        ),
        expand(
            f"{os.path.abspath(config['results_dir'])}/notebooks/{{sample}}_preprocessing.ipynb",
            sample=get_samples()
        ),
        f"{os.path.abspath(config['results_dir'])}/combined_preprocessing.ipynb",
        f"{os.path.abspath(config['results_dir'])}/combined_preprocessing.html",
        f"{os.path.abspath(config['results_dir'])}/combined_processed_adata.h5ad",


def get_reference_file():
    if config['organism'] == 'human':
        # return "/ref/cellranger/refdata-gex-GRCh38-2024-A"
        return "$(cellranger_human_reference)"
    elif config['organism'] == 'mouse':
        # return "/ref/cellranger/refdata-gex-GRCm39-2024-A"
        return "$(cellranger_mouse_reference)"
    else:
        raise ValueError("Invalid mode specified. Must be either 'three-prime' or 'flex'.")


def get_flex_probeset():
    if config['organism'] == 'human':
        # return "/ref/cellranger/Chromium_Human_Transcriptome_Probe_Set_v1.1.0_GRCh38-2024-A.csv"
        return "$(cellranger_human_flex_probe_set)"
    elif config['organism'] == 'mouse':
        # return "/ref/cellranger/Chromium_Mouse_Transcriptome_Probe_Set_v1.1.1_GRCm39-2024-A.csv"
        return "$(cellranger_mouse_flex_probe_set)"
    else:
        raise ValueError("Invalid organism specified. Must be either 'human' or 'mouse'.")


def get_merged_dir():
    return os.path.abspath(f"{config['results_dir']}/merged")


def get_fastp_dir():
    return os.path.abspath(f"{config['results_dir']}/fastp")


def strip_s_suffix(name: str) -> str:
    """Return Cell Ranger sample ID (everything before _S\d+)."""
    return re.sub(r"_S\d+$", "", name)


if config['mode'] == 'three-prime':
    # For the three-prime mode
    rule cellranger:
        container: f"{config['workflow_dir']}/docker/10xrangers/10x_ranger_image.sif"
        shadow: "copy-minimal"  # Required when using .sif
        input:
            r1=f"{get_merged_dir()}/{{sample}}_L001_R1_001.fastq.gz",
            r2=f"{get_merged_dir()}/{{sample}}_L001_R2_001.fastq.gz",
        output:
            cellranger_output=directory(f"{os.path.abspath(config['results_dir'])}/{config['cellranger_output']}/{{sample}}")
        params:
            merged_dir=get_merged_dir(),
            reference=get_reference_file(),
            sample_name=lambda wc: strip_s_suffix(wc.sample)
        log:
            f"{os.path.abspath(config['results_dir'])}/{config['cellranger_output']}/{{sample}}/_log"
        threads: 16
        shell:
            """
            . /bin/reference_functions.sh  # Load the reference environment variables
            cellranger count \
            --id {wildcards.sample} \
            --output-dir {output.cellranger_output} \
            --transcriptome {params.reference} \
            --fastqs {params.merged_dir} \
            --sample {params.sample_name} \
            --create-bam true \
            --localcores {threads}
            """
elif config['mode'] == 'flex':
    rule generate_multi_csv:  # Create the csv file input for the cellranger multi command
        container: f"{config['workflow_dir']}/docker/10xrangers/10x_ranger_image.sif"
        shadow: "copy-minimal"  # Required when using .sif
        input:
            r1=f"{get_merged_dir()}/{{sample}}_L001_R1_001.fastq.gz",
            r2=f"{get_merged_dir()}/{{sample}}_L001_R2_001.fastq.gz",
        output:
            multi_csv_input="multi_csv_input.csv"
        params:
            merged_dir=get_merged_dir(),
            reference=get_reference_file(),
            probeset=get_flex_probeset(),
            sample_name=lambda wc: strip_s_suffix(wc.sample)
        shell:
            """
            . /bin/reference_functions.sh  # Load the reference environment variables
            echo "[gene-expression]" >> {output.multi_csv_input}
            echo "reference,{params.reference}" >> {output.multi_csv_input}
            echo "create-bam,true" >> {output.multi_csv_input}
            echo "probe-set,{params.probeset}" >> {output.multi_csv_input}
            echo "[libraries]" >> {output.multi_csv_input}
            echo "fastq-id,{params.sample_name}" >> {output.multi_csv_input}
            echo "fastqs,{params.merged_dir}" >> {output.multi_csv_input}
            echo "feature_types,Gene Expression" >> {output.multi_csv_input}
            """

    rule cellranger:
        container: f"{config['workflow_dir']}/docker/10xrangers/10x_ranger_image.sif"
        shadow: "copy-minimal"  # Required when using .sif
        input:
            r1=f"{get_merged_dir()}/{{sample}}_L001_R1_001.fastq.gz",
            r2=f"{get_merged_dir()}/{{sample}}_L001_R2_001.fastq.gz",
            multi_csv_input="multi_csv_input.csv"
        output:
            cellranger_output=directory(f"{os.path.abspath(config['results_dir'])}/{config['cellranger_output']}/{{sample}}")
        log:
            f"{os.path.abspath(config['results_dir'])}/{config['cellranger_output']}/{{sample}}/_log"
        threads: 16
        shell:
            """
            . /bin/reference_functions.sh  # Load the reference environment variables
            cellranger multi \
            --id {wildcards.sample} \
            --csv {input.multi_csv_input} \
            --output-dir {output.cellranger_output} \
            --localcores {threads}
            """
else:
    raise ValueError("Invalid mode specified. Must be either 'three-prime' or 'flex'.")


rule multiqc:
    container: f"{config['workflow_dir']}/docker/multiqc/multiqc_image.sif"
    shadow: "copy-minimal"  # Required when using .sif
    input:
        tenx_output=f"{os.path.abspath(config['results_dir'])}/{config['cellranger_output']}/{{sample}}",
        r1=f"{get_merged_dir()}/{{sample}}_L001_R1_001.fastq.gz",
        r2=f"{get_merged_dir()}/{{sample}}_L001_R2_001.fastq.gz",
        fastp_report_json=f"{get_fastp_dir()}/qc/{{sample}}_report.json",
        fastp_report_html=f"{get_fastp_dir()}/qc/{{sample}}_report.html"
    output:
        outdir=directory(f"{os.path.abspath(config['results_dir'])}/multiqc/{{sample}}/"),
        outreport=f"{os.path.abspath(config['results_dir'])}/multiqc/{{sample}}/multiqc_report.html"
    params:
        indir=directory(os.path.abspath(config["results_dir"]))
    shell:
        "multiqc --outdir {output.outdir} {params.indir}"


rule copy_notebook:
    input:
        notebook1 = f"{config['workflow_dir']}/notebooks/scrnaseq_preprocessing_pt1.ipynb",
        notebook2 = f"{config['workflow_dir']}/notebooks/scrnaseq_preprocessing_pt2.ipynb",
        notebook3 = f"{config['workflow_dir']}/notebooks/doublet_detection.ipynb"
    output:
        temp_notebook1 = temp(
            f"{os.path.abspath(config['results_dir'])}/notebooks/scrnaseq_preprocessing_pt1.ipynb"
        ),
        temp_notebook2 = temp(
            f"{os.path.abspath(config['results_dir'])}/notebooks/scrnaseq_preprocessing_pt2.ipynb"
        ),
        temp_notebook3 = temp(
            f"{os.path.abspath(config['results_dir'])}/notebooks/doublet_detection.ipynb"
        )
    shell:
        """
        mkdir -p $(dirname {output.temp_notebook1})
        cp {input.notebook1} {output.temp_notebook1}
        mkdir -p $(dirname {output.temp_notebook2})
        cp {input.notebook2} {output.temp_notebook2}
        mkdir -p $(dirname {output.temp_notebook3})
        cp {input.notebook3} {output.temp_notebook3}
        """

rule make_h5ad:
    container:
         f"{config['workflow_dir']}/docker/generated/scanpy-environment.sif"
    shadow: "copy-minimal"  # Required when using .sif
    conda:
        f"{config['workflow_dir']}/conda/scanpy-environment.yml"
    input:
        tenx_output=f"{os.path.abspath(config['results_dir'])}/{config['cellranger_output']}/{{sample}}",
        sample_name=lambda wc: strip_s_suffix(wc.sample)
    output:
        output_file=temp(f"{os.path.abspath(config['results_dir'])}/notebooks/{{sample}}_converted_adata.h5ad")
    script: f"{config['workflow_dir']}/workflows/scrnaseq/convert_10x_directory_to_h5ad.py"


rule preprocess_scrna:  # Initial QC/Filtering
    container:
         f"{config['workflow_dir']}/docker/generated/scanpy-environment.sif"
    shadow: "copy-minimal"  # Required when using .sif
    conda:
        f"{config['workflow_dir']}/conda/scanpy-environment.yml"
    input:
        input_file=f"{os.path.abspath(config['results_dir'])}/notebooks/{{sample}}_converted_adata.h5ad",
        notebook=f"{os.path.abspath(config['results_dir'])}/notebooks/scrnaseq_preprocessing_pt1.ipynb"
    output:
        notebook_out=f"{os.path.abspath(config['results_dir'])}/notebooks/{{sample}}_preprocessing.ipynb",
        html_out=f"{os.path.abspath(config['results_dir'])}/notebooks/{{sample}}_preprocessing.html",
        output_file=f"{os.path.abspath(config['results_dir'])}/notebooks/{{sample}}_processed_adata.h5ad"
    params:
        outlier_threshold=config['outlier_threshold'],  # Parameter for MAD-based outlier detection
    log:
        f"{os.path.abspath(config['results_dir'])}/logs/{{sample}}_preprocessing.log"
    # Papermill can sometimes compete with ports if there are multiple papermill jobs running
    retries: 5
    shell:
        """
        export SNAKEMAKE_H5AD_FILE={input.input_file}
        export SNAKEMAKE_OUTLIER_THRESHOLD={params.outlier_threshold}
        export SNAKEMAKE_PROCESSED_FILENAME={output.output_file}
        papermill --stdout-file {log} --stderr-file {log} --no-progress-bar {input.notebook} {output.notebook_out}
        jupyter nbconvert --to html {output.notebook_out} >> {log} 2>&1
        """


rule combine_samples:  # Combine multiple .h5ad files into a single file if there are multiple samples
    container: f"{config['workflow_dir']}/docker/generated/scanpy-environment.sif"
    shadow: "copy-minimal"  # Required when using .sif
    conda:
        f"{config['workflow_dir']}/conda/scanpy-environment.yml"
    input:
        h5ad_files=expand(
            f"{os.path.abspath(config['results_dir'])}/notebooks/{{sample}}_processed_adata.h5ad",
            sample=get_samples()
        )
    output:
        combined_h5ad=temp(f"{os.path.abspath(config['results_dir'])}/temp_combined_processed_adata.h5ad")
    script: f"{config['workflow_dir']}/workflows/scrnaseq/concatenate_h5ads.py"


rule doublet_detection:  # Detect doublets using scDblFinder
    container: f"{config['workflow_dir']}/docker/generated/scdblfinder-environment.sif"
    shadow: "copy-minimal"  # Required when using .sif
    conda:
        f"{config['workflow_dir']}/conda/scdblfinder-environment.yml"
    input:
        input_file=f"{os.path.abspath(config['results_dir'])}/temp_combined_processed_adata.h5ad",
        notebook=f"{os.path.abspath(config['results_dir'])}/notebooks/doublet_detection.ipynb"
    output:
        output_file=temp(f"{os.path.abspath(config['results_dir'])}/doublet_table.csv"),
        notebook_out=f"{os.path.abspath(config['results_dir'])}/doublet_detection.ipynb",
        notebook_html=f"{os.path.abspath(config['results_dir'])}/doublet_detection.html"
    log:
        f"{os.path.abspath(config['results_dir'])}/logs/doublet_detection.log"
    retries: 5
    shell:
        """
        export SNAKEMAKE_INPUT_H5AD={input.input_file}
        export SNAKEMAKE_OUTPUT_TABLE={output.output_file}
        papermill --stdout-file {log} --stderr-file {log} --no-progress-bar {input.notebook} {output.notebook_out}
        jupyter nbconvert --to html {output.notebook_out} >> {log} 2>&1
        """

rule preprocess_combined_scrna:  # Final basic analysis (HVGs, PCA, UMAP, etc.)
    container: f"{config['workflow_dir']}/docker/generated/scanpy-environment.sif"
    shadow: "copy-minimal"  # Required when using .sif
    conda:
        f"{config['workflow_dir']}/conda/scanpy-environment.yml"
    input:
        input_file=f"{os.path.abspath(config['results_dir'])}/temp_combined_processed_adata.h5ad",
        notebook=f"{os.path.abspath(config['results_dir'])}/notebooks/scrnaseq_preprocessing_pt2.ipynb",
        doublet_file=f"{os.path.abspath(config['results_dir'])}/doublet_table.csv"
    output:
        notebook_out=f"{os.path.abspath(config['results_dir'])}/combined_preprocessing.ipynb",
        html_out=f"{os.path.abspath(config['results_dir'])}/combined_preprocessing.html",
        output_file=f"{os.path.abspath(config['results_dir'])}/combined_processed_adata.h5ad"
    params:
        n_hvgs=config['n_hvgs'] # Number of highly variable genes to select
    log:
        f"{os.path.abspath(config['results_dir'])}/logs/combined_preprocessing.log"
    # Papermill can sometimes compete with ports if there are multiple papermill jobs running
    retries: 5
    shell:
        """
        export SNAKEMAKE_H5AD_FILE={input.input_file}
        export SNAKEMAKE_N_HVGS={params.n_hvgs}
        export SNAKEMAKE_PROCESSED_FILENAME={output.output_file}
        export SNAKEMAKE_DOUBLET_FILE={input.doublet_file}
        papermill --stdout-file {log} --stderr-file {log} --no-progress-bar {input.notebook} {output.notebook_out}
        jupyter nbconvert --to html {output.notebook_out} >> {log} 2>&1
        """
